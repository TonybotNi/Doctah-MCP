#!/usr/bin/env python3
"""
PRTS.wiki æŸ¥è¯¢åŠŸèƒ½æ¨¡å—
æä¾›æ˜æ—¥æ–¹èˆŸ PRTS.wiki ç½‘ç«™çš„æ•°æ®æŸ¥è¯¢å’Œè§£æåŠŸèƒ½
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import quote, unquote
import httpx
from bs4 import BeautifulSoup, Comment
import re

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# PRTS.wiki åŸºç¡€é…ç½®
BASE_URL = "https://prts.wiki"
SEARCH_API = f"{BASE_URL}/api.php"



class PRTSWikiClient:
    """PRTS.wiki å®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.session = httpx.AsyncClient(
            timeout=httpx.Timeout(30.0),
            headers={
                "User-Agent": "PRTS-MCP-Server/1.0 (https://github.com/example/prts-mcp)"
            }
        )
    
    async def close(self):
        """å…³é—­HTTPä¼šè¯"""
        await self.session.aclose()
    
    async def search_pages(self, query: str, limit: int = 10) -> List[Dict[str, str]]:
        """æœç´¢é¡µé¢"""
        try:
            params = {
                "action": "query",
                "format": "json",
                "list": "search",
                "srsearch": query,
                "srlimit": limit,
                "srprop": "title|snippet"
            }
            
            response = await self.session.get(SEARCH_API, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = []
            if "query" in data and "search" in data["query"]:
                for item in data["query"]["search"]:
                    results.append({
                        "title": item["title"],
                        "snippet": item.get("snippet", ""),
                        "url": f"{BASE_URL}/w/{quote(item['title'])}"
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢é¡µé¢å¤±è´¥: {e}")
            return []
    
    async def get_page_html(self, title: str) -> str:
        """è·å–é¡µé¢HTMLå†…å®¹"""
        try:
            url = f"{BASE_URL}/w/{quote(title)}"
            response = await self.session.get(url)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"è·å–é¡µé¢HTMLå¤±è´¥: {e}")
            return ""
    
    async def get_page_content(self, title: str) -> str:
        """è·å–é¡µé¢çº¯æ–‡æœ¬å†…å®¹"""
        try:
            params = {
                "action": "query",
                "format": "json",
                "titles": title,
                "prop": "extracts",
                "exintro": False,
                "explaintext": True,
                "exsectionformat": "plain"
            }
            
            response = await self.session.get(SEARCH_API, params=params)
            response.raise_for_status()
            data = response.json()
            
            pages = data.get("query", {}).get("pages", {})
            for page_id, page_data in pages.items():
                if "extract" in page_data:
                    return page_data["extract"]
            
            return ""
            
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            return ""

    def extract_text_from_cell(self, cell) -> str:
        """ä»è¡¨æ ¼å•å…ƒæ ¼ä¸­æå–çº¯æ–‡æœ¬ï¼Œæ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šæ ‡è®°"""
        if not cell:
            return ""
        
        # ç§»é™¤æ³¨é‡Š
        for comment in cell.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
        
        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
        for tag in cell.find_all(['script', 'style']):
            tag.decompose()
        
        # ç§»é™¤ç‰¹å®šçš„spanæ ‡ç­¾ï¼ˆç®—æ³•æ ‡è®°ç­‰ï¼‰
        try:
            for span in cell.find_all('span'):
                if span and hasattr(span, 'get') and span.get('style'):
                    style = span.get('style', '')
                    if style and 'display:none' in style:
                        span.decompose()
        except Exception:
            pass  # å¿½ç•¥å¤„ç†spanæ ‡ç­¾æ—¶çš„é”™è¯¯
        
        # è·å–æ–‡æœ¬
        text = cell.get_text(separator=' ', strip=True)
        
        # æ¸…ç†ç‰¹æ®Šæ ‡è®°
        text = re.sub(r'æ˜¾ç¤ºç®—æ³•.*?(?=\s|$)', '', text)
        text = re.sub(r'ç›´æ¥ä¹˜ç®—.*?(?=\s|$)', '', text)
        text = re.sub(r'é‡Œå±äºå åŠ .*?(?=\s|$)', '', text)
        text = re.sub(r'ä¸º.*?ä¹˜ç®—', '', text)
        text = re.sub(r'\[\s*æ³¨\s*\d+\s*\]', '', text)  # ç§»é™¤æ³¨é‡Šæ ‡è®°
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        text = re.sub(r'^\s*ï¼š\s*', '', text)  # ç§»é™¤å¼€å¤´çš„å†’å·
        
        return text.strip()

    def extract_table_of_contents(self, soup: BeautifulSoup) -> Dict[str, Dict]:
        """æå–é¡µé¢ç›®å½•ç»“æ„"""
        toc = {}
        
        # æŸ¥æ‰¾ç›®å½•å®¹å™¨
        toc_div = soup.find('div', {'id': 'toc'}) or soup.find('div', class_='toc')
        
        if toc_div:
            # ä»ç›®å½•ä¸­æå–
            for link in toc_div.find_all('a'):
                href = link.get('href', '')
                if href.startswith('#'):
                    section_id = href[1:]
                    section_title = link.get_text(strip=True)
                    toc[section_id] = {
                        'title': section_title,
                        'level': len(link.find_parents('li', recursive=False)),
                        'anchor': section_id
                    }
        else:
            # ä»æ ‡é¢˜ä¸­æå–ç›®å½•
            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                if heading.get('id'):
                    section_id = heading.get('id')
                    section_title = heading.get_text(strip=True)
                    level = int(heading.name[1])  # h1->1, h2->2, etc.
                    toc[section_id] = {
                        'title': section_title,
                        'level': level,
                        'anchor': section_id
                    }
        
        return toc

    def extract_section_content(self, soup: BeautifulSoup, section_id: str, next_section_id: str = None) -> str:
        """æå–æŒ‡å®šç« èŠ‚çš„å†…å®¹"""
        content = []
        
        # æ‰¾åˆ°ç« èŠ‚æ ‡é¢˜
        section_heading = soup.find(id=section_id)
        if not section_heading:
            return ""
        
        # å¦‚æœä¸æ˜¯æ ‡é¢˜æ ‡ç­¾ï¼ŒæŸ¥æ‰¾å…¶çˆ¶çº§æ ‡é¢˜
        if section_heading.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            section_heading = section_heading.find_parent(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
        if not section_heading:
            return ""
        
        # è·å–ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§çš„æ ‡é¢˜ä½œä¸ºè¾¹ç•Œ
        current_level = int(section_heading.name[1])
        next_element = section_heading.next_sibling
        
        while next_element:
            if hasattr(next_element, 'name'):
                if next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    next_level = int(next_element.name[1])
                    if next_level <= current_level:
                        break
                
                # æå–è¡¨æ ¼å†…å®¹
                if next_element.name == 'table':
                    table_content = self.extract_table_content(next_element)
                    if table_content:
                        content.append(table_content)
                
                # æå–å…¶ä»–å†…å®¹
                elif next_element.name in ['p', 'div', 'ul', 'ol', 'dl']:
                    text = self.extract_text_from_cell(next_element)
                    if text:
                        content.append(text)
            
            next_element = next_element.next_sibling
        
        return '\n\n'.join(content) if content else ""

    def extract_table_content(self, table) -> str:
        """æå–è¡¨æ ¼å†…å®¹ä¸ºmarkdownæ ¼å¼"""
        if not table:
            return ""
        
        content = []
        rows = table.find_all('tr')
        
        for row in rows:
            cells = row.find_all(['th', 'td'])
            if cells:
                row_data = []
                for cell in cells:
                    cell_text = self.extract_text_from_cell(cell)
                    row_data.append(cell_text)
                
                if any(row_data):  # åªæœ‰å½“è¡Œä¸­æœ‰å†…å®¹æ—¶æ‰æ·»åŠ 
                    content.append(' | '.join(row_data))
        
        return '\n'.join(content) if content else ""

    async def parse_operator_complete(self, title: str, sections: Optional[List[str]] = None) -> Dict[str, Any]:
        """å®Œæ•´è§£æå¹²å‘˜ä¿¡æ¯ï¼Œæ”¯æŒç« èŠ‚è¿‡æ»¤"""
        html = await self.get_page_html(title)
        if not html:
            return {}
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•Œäººé¡µé¢
        if self._is_enemy_page(soup, title):
            return {
                'type': 'enemy',
                'name': title,
                'url': f"{BASE_URL}/w/{quote(title)}",
                'message': 'è¯¥é¡µé¢ä¸ºæ•Œäººé¡µé¢ï¼Œè¯·ä½¿ç”¨æ•ŒäººæŸ¥è¯¢åŠŸèƒ½'
            }
        
        # æå–ç›®å½•ç»“æ„
        toc = self.extract_table_of_contents(soup)
        
        # åŸºç¡€æ•°æ®ç»“æ„
        operator_data = {
            'type': 'operator',
            'name': title,
            'url': f"{BASE_URL}/w/{quote(title)}",
            'table_of_contents': toc,
            'sections': {}
        }
        
        # å®šä¹‰ç« èŠ‚æ˜ å°„
        section_mapping = {
            'å¹²å‘˜ä¿¡æ¯': 'operator_info',
            'ç‰¹æ€§': 'characteristics', 
            'è·å¾—æ–¹å¼': 'acquisition',
            'å±æ€§': 'attributes',
            'æ”»å‡»èŒƒå›´': 'attack_range',
            'å¤©èµ‹': 'talents',
            'æ½œèƒ½æå‡': 'potential',
            'æŠ€èƒ½': 'skills',
            'åå‹¤æŠ€èƒ½': 'base_skills',
            'ç²¾è‹±åŒ–ææ–™': 'elite_materials',
            'æŠ€èƒ½å‡çº§ææ–™': 'skill_materials',
            'æ¨¡ç»„': 'modules',
            'ç›¸å…³é“å…·': 'related_items',
            'å¹²å‘˜æ¡£æ¡ˆ': 'operator_record',
            'è¯­éŸ³è®°å½•': 'voice_records',
            'å¹²å‘˜å¯†å½•': 'operator_files',
            'æ‚–è®ºæ¨¡æ‹Ÿ': 'paradox_simulation',
            'å¹²å‘˜æ¨¡å‹': 'operator_model',
            'æ³¨é‡Šä¸é“¾æ¥': 'notes_and_links'
        }
        
        try:
            # å¦‚æœæŒ‡å®šäº†ç« èŠ‚ï¼Œåªè§£ææŒ‡å®šç« èŠ‚
            if sections:
                target_sections = sections
            else:
                # è§£ææ‰€æœ‰å¯è¯†åˆ«çš„ç« èŠ‚ï¼Œè·³è¿‡ä¸éœ€è¦çš„ç« èŠ‚
                skip_sections = {'æ³¨é‡Šä¸é“¾æ¥', 'å¹²å‘˜æ¨¡å‹'}
                target_sections = [s for s in section_mapping.keys() 
                                 if not any(skip_section in s for skip_section in skip_sections)]
            
            for section_title in target_sections:
                if section_title in section_mapping:
                    section_key = section_mapping[section_title]
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„ç« èŠ‚ID
                    section_id = None
                    for toc_id, toc_info in toc.items():
                        # æ›´ç²¾ç¡®çš„åŒ¹é…é€»è¾‘
                        if (section_title == toc_id or 
                            section_title in toc_info['title'] or 
                            toc_info['title'].endswith(section_title) or
                            toc_id == section_title):
                            section_id = toc_id
                            break
                    
                    if section_id:
                        content = self.extract_section_content(soup, section_id)
                        if content:
                            operator_data['sections'][section_key] = {
                                'title': section_title,
                                'content': content
                            }
                    
                    # ä½¿ç”¨åŸæœ‰çš„ä¸“é—¨è§£ææ–¹æ³•ä½œä¸ºå¤‡é€‰
                    if section_key not in operator_data['sections']:
                        if section_title == 'å±æ€§':
                            attr_data = {}
                            self._extract_attributes(soup, attr_data)
                            if attr_data.get('attributes'):
                                operator_data['sections'][section_key] = {
                                    'title': section_title,
                                    'content': self._format_attributes(attr_data['attributes'])
                                }
                        elif section_title == 'å¤©èµ‹':
                            talent_data = {'talents': []}
                            self._extract_talents(soup, talent_data)
                            if talent_data['talents']:
                                operator_data['sections'][section_key] = {
                                    'title': section_title,
                                    'content': self._format_talents(talent_data['talents'])
                                }
                        elif section_title == 'æŠ€èƒ½':
                            skill_data = {'skills': []}
                            self._extract_skills(soup, skill_data)
                            if skill_data['skills']:
                                operator_data['sections'][section_key] = {
                                    'title': section_title,
                                    'content': self._format_skills(skill_data['skills'])
                                }
                        elif section_title == 'ç‰¹æ€§':
                            char_data = {}
                            self._extract_characteristics(soup, char_data)
                            if char_data.get('characteristics'):
                                operator_data['sections'][section_key] = {
                                    'title': section_title,
                                    'content': char_data['characteristics']
                                }
            
            # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆæ€»æ˜¯åŒ…å«ï¼‰
            basic_info = {}
            self._extract_basic_info(soup, basic_info)
            if basic_info.get('basic_info'):
                operator_data['basic_info'] = basic_info['basic_info']
                
            # æå–èŒä¸šå’Œç¨€æœ‰åº¦ï¼ˆæ€»æ˜¯åŒ…å«ï¼‰  
            self._extract_profession_rarity(soup, operator_data)
                
        except Exception as e:
            logger.error(f"è§£æå¹²å‘˜å®Œæ•´ä¿¡æ¯å¤±è´¥: {e}")
        
        return operator_data

    def _format_attributes(self, attributes: Dict) -> str:
        """æ ¼å¼åŒ–å±æ€§æ•°æ®"""
        content = []
        for attr_name, attr_data in attributes.items():
            content.append(f"### {attr_name}")
            for key, value in attr_data.items():
                content.append(f"- **{key}**: {value}")
            content.append("")
        return '\n'.join(content)

    def _format_talents(self, talents: List) -> str:
        """æ ¼å¼åŒ–å¤©èµ‹æ•°æ®"""
        content = []
        for i, talent in enumerate(talents, 1):
            content.append(f"### å¤©èµ‹ {i}: {talent.get('name', 'æœªçŸ¥')}")
            if talent.get('condition'):
                content.append(f"**è§£é”æ¡ä»¶**: {talent['condition']}")
            if talent.get('description'):
                content.append(f"**æ•ˆæœ**: {talent['description']}")
            content.append("")
        return '\n'.join(content)

    def _format_skills(self, skills: List) -> str:
        """æ ¼å¼åŒ–æŠ€èƒ½æ•°æ®"""
        content = []
        for i, skill in enumerate(skills, 1):
            content.append(f"### æŠ€èƒ½ {i}: {skill.get('name', 'æœªçŸ¥')}")
            if skill.get('type'):
                content.append(f"**ç±»å‹**: {skill['type']}")
            if skill.get('description'):
                content.append(f"**æ•ˆæœ**: {skill['description']}")
            if skill.get('levels'):
                content.append("**ç­‰çº§æ•°æ®**:")
                for level, level_data in skill['levels'].items():
                    content.append(f"- {level}: {level_data}")
            content.append("")
        return '\n'.join(content)

    async def parse_operator_detailed(self, title: str) -> Dict[str, Any]:
        """è¯¦ç»†è§£æå¹²å‘˜ä¿¡æ¯ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return await self.parse_operator_complete(title)

    def _extract_basic_info(self, soup: BeautifulSoup, operator_data: Dict):
        """æå–åŸºæœ¬ä¿¡æ¯"""
        basic_info = {}
        
        # æŸ¥æ‰¾å±æ€§è¡¨æ ¼
        for table in soup.find_all('table'):
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['th', 'td'])
                if len(cells) >= 2:
                    key = self.extract_text_from_cell(cells[0])
                    value = self.extract_text_from_cell(cells[1])
                    
                    if key and value:
                        # æ ‡å‡†åŒ–å­—æ®µå
                        if 'å†éƒ¨ç½²' in key or 'éƒ¨ç½²æ—¶é—´' in key:
                            basic_info['å†éƒ¨ç½²æ—¶é—´'] = value
                        elif 'é˜»æŒ¡' in key:
                            basic_info['é˜»æŒ¡æ•°'] = value
                        elif 'æ‰€å±åŠ¿åŠ›' in key and 'éšè—' not in key:
                            basic_info['æ‰€å±åŠ¿åŠ›'] = value
                        elif 'éšè—åŠ¿åŠ›' in key:
                            basic_info['éšè—åŠ¿åŠ›ä»…åœ¨æˆ˜æ–—ä¸­æ‰€ä½¿ç”¨çš„æ•°æ®'] = value
                        elif 'æ”»å‡»é—´éš”' in key:
                            basic_info['æ”»å‡»é—´éš”'] = value
                        elif 'éƒ¨ç½²è´¹ç”¨' in key:
                            basic_info['éƒ¨ç½²è´¹ç”¨'] = value
        
        operator_data['basic_info'] = basic_info

    def _extract_attributes(self, soup: BeautifulSoup, operator_data: Dict):
        """æå–å±æ€§æ•°æ®"""
        attributes = {}
        
        # æŸ¥æ‰¾å±æ€§è¡¨æ ¼
        for table in soup.find_all('table'):
            rows = table.find_all('tr')
            if not rows:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯å±æ€§è¡¨æ ¼
            header_row = rows[0]
            header_cells = header_row.find_all(['th', 'td'])
            header_text = ' '.join([self.extract_text_from_cell(cell) for cell in header_cells])
            
            if 'ç²¾è‹±' in header_text and ('ç”Ÿå‘½' in header_text or 'æ”»å‡»' in header_text):
                # è¿™æ˜¯ä¸€ä¸ªå±æ€§è¡¨æ ¼
                for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                    cells = row.find_all(['th', 'td'])
                    if len(cells) >= 2:
                        attr_name = self.extract_text_from_cell(cells[0])
                        
                        if attr_name and any(keyword in attr_name for keyword in ['ç”Ÿå‘½', 'æ”»å‡»', 'é˜²å¾¡', 'æ³•æœ¯æŠ—æ€§']):
                            attr_data = {}
                            for i, cell in enumerate(cells[1:], 1):
                                value = self.extract_text_from_cell(cell)
                                if value and value != '-':
                                    if i == 1:
                                        attr_data['ç²¾è‹±0 1çº§ä¸åŒ…æ‹¬ä¿¡èµ–åŠæ½œèƒ½åŠ æˆ'] = value
                                    elif i == 2:
                                        attr_data['ç²¾è‹±0 æ»¡çº§ä¸åŒ…æ‹¬ä¿¡èµ–åŠæ½œèƒ½åŠ æˆ'] = value
                                    elif i == 3:
                                        attr_data['ç²¾è‹±1 æ»¡çº§ä¸åŒ…æ‹¬ä¿¡èµ–åŠæ½œèƒ½åŠ æˆ'] = value
                                    elif i == 4:
                                        attr_data['ç²¾è‹±2 æ»¡çº§ä¸åŒ…æ‹¬ä¿¡èµ–åŠæ½œèƒ½åŠ æˆ'] = value
                                    elif i == 5 and 'ä¿¡èµ–' not in value:
                                        attr_data['ä¿¡èµ–åŠ æˆä¸Šé™'] = value
                            
                            if attr_data:
                                attributes[attr_name] = attr_data
        
        operator_data['attributes'] = attributes

    def _extract_characteristics(self, soup: BeautifulSoup, operator_data: Dict):
        """æå–ç‰¹æ€§ä¿¡æ¯"""
        characteristics = ""
        
        # æŸ¥æ‰¾ç‰¹æ€§è¡¨æ ¼
        for table in soup.find_all('table'):
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['th', 'td'])
                if len(cells) >= 2:
                    header = self.extract_text_from_cell(cells[0])
                    if 'åˆ†æ”¯' in header or 'ç‰¹æ€§' in header:
                        for cell in cells[1:]:
                            text = self.extract_text_from_cell(cell)
                            if text and 'ä¼˜å…ˆæ”»å‡»' in text:
                                characteristics = text
                                break
                        if characteristics:
                            break
        
        operator_data['characteristics'] = characteristics

    def _extract_talents(self, soup: BeautifulSoup, operator_data: Dict):
        """æå–å¤©èµ‹ä¿¡æ¯"""
        talents = []
        seen_talents = set()
        
        # æŸ¥æ‰¾å¤©èµ‹è¡¨æ ¼
        for table in soup.find_all('table'):
            rows = table.find_all('tr')
            if not rows:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤©èµ‹è¡¨æ ¼
            for row in rows:
                cells = row.find_all(['th', 'td'])
                if len(cells) >= 3:
                    first_cell = self.extract_text_from_cell(cells[0])
                    if 'å¤©èµ‹' in first_cell and ('ç¬¬' in first_cell or '1' in first_cell or '2' in first_cell):
                        self._parse_talent_table(table, operator_data)
                        break
        
        # ä»å¤„ç†åçš„æ•°æ®ä¸­å»é‡
        if 'talents' in operator_data:
            for talent in operator_data['talents']:
                talent_key = f"{talent.get('name', '')}-{talent.get('condition', '')}"
                if talent_key not in seen_talents:
                    seen_talents.add(talent_key)
                    talents.append(talent)
        
        operator_data['talents'] = talents

    def _parse_talent_table(self, table, operator_data: Dict):
        """è§£æå¤©èµ‹è¡¨æ ¼"""
        if 'talents' not in operator_data:
            operator_data['talents'] = []
        
        rows = table.find_all('tr')
        for row in rows:
            cells = row.find_all(['th', 'td'])
            if len(cells) >= 3:
                first_cell = self.extract_text_from_cell(cells[0])
                
                if 'å¤©èµ‹' in first_cell:
                    # æå–å¤©èµ‹åç§°
                    name_text = self.extract_text_from_cell(cells[1])
                    condition_text = self.extract_text_from_cell(cells[2])
                    
                    # æŸ¥æ‰¾æè¿° - å¯èƒ½åœ¨ç¬¬4åˆ—æˆ–åç»­è¡Œ
                    description = ""
                    if len(cells) > 3:
                        description = self.extract_text_from_cell(cells[3])
                    
                    if name_text and condition_text:
                        talent = {
                            'name': name_text,
                            'condition': condition_text,
                            'description': description
                        }
                        operator_data['talents'].append(talent)

    def _extract_skills(self, soup: BeautifulSoup, operator_data: Dict):
        """æå–æŠ€èƒ½ä¿¡æ¯"""
        skills = []
        seen_skills = set()
        
        # æŸ¥æ‰¾æŠ€èƒ½è¡¨æ ¼
        for table in soup.find_all('table'):
            rows = table.find_all('tr')
            if not rows:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŠ€èƒ½è¡¨æ ¼
            header_text = ""
            for row in rows[:2]:  # æ£€æŸ¥å‰ä¸¤è¡Œ
                cells = row.find_all(['th', 'td'])
                for cell in cells:
                    cell_text = self.extract_text_from_cell(cell)
                    if 'æŠ€èƒ½' in cell_text and ('åç§°' in cell_text or 'ç­‰çº§' in cell_text or 'æè¿°' in cell_text):
                        header_text += cell_text + " "
            
            if 'æŠ€èƒ½' in header_text:
                self._parse_skill_table(table, operator_data)
        
        # ä»å¤„ç†åçš„æ•°æ®ä¸­å»é‡
        if 'skills' in operator_data:
            for skill in operator_data['skills']:
                skill_key = f"{skill.get('name', '')}-{skill.get('type', '')}"
                if skill_key not in seen_skills:
                    seen_skills.add(skill_key)
                    skills.append(skill)
        
        operator_data['skills'] = skills

    def _parse_skill_table(self, table, operator_data: Dict):
        """è§£ææŠ€èƒ½è¡¨æ ¼"""
        if 'skills' not in operator_data:
            operator_data['skills'] = []
        
        rows = table.find_all('tr')
        current_skill = None
        
        for row in rows:
            cells = row.find_all(['th', 'td'])
            if not cells:
                continue
            
            # å°è¯•è¯†åˆ«æŠ€èƒ½è¡Œ
            first_cell = self.extract_text_from_cell(cells[0])
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æŠ€èƒ½çš„å¼€å§‹
            if 'æŠ€èƒ½' in first_cell and ('åç§°' in first_cell or len(cells) >= 3):
                if len(cells) >= 2:
                    skill_name = self.extract_text_from_cell(cells[1])
                    if skill_name and skill_name not in ['åç§°', 'æŠ€èƒ½åç§°']:
                        current_skill = {
                            'name': skill_name,
                            'type': '',
                            'description': '',
                            'levels': {}
                        }
                        
                        # æŸ¥æ‰¾æŠ€èƒ½ç±»å‹å’Œæè¿°
                        if len(cells) >= 3:
                            type_or_desc = self.extract_text_from_cell(cells[2])
                            if 'è‡ªåŠ¨å›å¤' in type_or_desc or 'æ‰‹åŠ¨è§¦å‘' in type_or_desc or 'è¢«åŠ¨' in type_or_desc:
                                current_skill['type'] = type_or_desc
                            else:
                                current_skill['description'] = type_or_desc
                        
                        if len(cells) >= 4:
                            current_skill['description'] = self.extract_text_from_cell(cells[3])
                        
                        operator_data['skills'].append(current_skill)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŠ€èƒ½ç­‰çº§æ•°æ®
            elif current_skill and first_cell and ('ç­‰çº§' in first_cell or first_cell.isdigit()):
                level_name = first_cell
                level_data = []
                
                for cell in cells[1:]:
                    value = self.extract_text_from_cell(cell)
                    if value:
                        level_data.append(value)
                
                if level_data:
                    current_skill['levels'][level_name] = ' | '.join(level_data)

    def _extract_profession_rarity(self, soup: BeautifulSoup, operator_data: Dict):
        """æå–èŒä¸šå’Œç¨€æœ‰åº¦ä¿¡æ¯"""
        # ä»é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡ä¸­æå–
        title_text = soup.find('h1', {'id': 'firstHeading'})
        if title_text:
            title = title_text.get_text(strip=True)
            
            # æå–ç¨€æœ‰åº¦ï¼ˆä»æ˜Ÿæ˜Ÿå›¾æ ‡ï¼‰
            star_imgs = soup.find_all('img', alt=re.compile(r'ç¨€æœ‰åº¦'))
            if star_imgs:
                for img in star_imgs:
                    alt_text = img.get('alt', '')
                    if 'æ˜Ÿ' in alt_text:
                        operator_data['rarity'] = alt_text
                        break
            
            # æå–èŒä¸šï¼ˆä»èŒä¸šå›¾æ ‡æˆ–æ–‡æœ¬ï¼‰
            profession_imgs = soup.find_all('img', alt=re.compile(r'(åŒ»ç–—|æœ¯å¸ˆ|ç‹™å‡»|é‡è£…|è¿‘å«|å…ˆé”‹|è¾…åŠ©|ç‰¹ç§)'))
            if profession_imgs:
                for img in profession_imgs:
                    alt_text = img.get('alt', '')
                    for prof in ['åŒ»ç–—', 'æœ¯å¸ˆ', 'ç‹™å‡»', 'é‡è£…', 'è¿‘å«', 'å…ˆé”‹', 'è¾…åŠ©', 'ç‰¹ç§']:
                        if prof in alt_text:
                            operator_data['profession'] = prof
                            break

    async def parse_enemy_complete(self, title: str, target_sections: Optional[List[str]] = None) -> Optional[Dict]:
        """è§£ææ•Œäººé¡µé¢çš„å®Œæ•´ä¿¡æ¯"""
        try:
            html = await self.get_page_html(title)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'lxml')
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•Œäººé¡µé¢
            if not self._is_enemy_page(soup, title):
                return None
            
            enemy_data = {
                'title': title,
                'type': 'enemy',
                'basic_info': {},
                'levels': {},
                'table_of_contents': {},
                'sections': {}
            }
            
            # æå–ç›®å½•
            toc = self.extract_table_of_contents(soup)
            enemy_data['table_of_contents'] = toc
            
            # æå–åŸºæœ¬ä¿¡æ¯
            self._extract_enemy_basic_info(soup, enemy_data)
            
            # æå–æ•Œäººç­‰çº§ä¿¡æ¯
            self._extract_enemy_levels(soup, enemy_data)
            
            # æ ¹æ®ç« èŠ‚éœ€æ±‚è¿‡æ»¤å†…å®¹
            if target_sections:
                filtered_sections = {}
                for section_id, section_info in toc.items():
                    section_title = section_info['title']
                    if any(target in section_title for target in target_sections):
                        content = self.extract_section_content(soup, section_id)
                        if content:
                            filtered_sections[section_id] = {
                                'title': section_title,
                                'content': content
                            }
                enemy_data['sections'] = filtered_sections
            else:
                # æå–æ‰€æœ‰ç« èŠ‚å†…å®¹
                all_sections = {}
                skip_sections = {'æ•Œäººæ¨¡å‹', 'å¯¼èˆªèœå•'}  # è·³è¿‡ä¸éœ€è¦çš„ç« èŠ‚
                
                for section_id, section_info in toc.items():
                    section_title = section_info['title']
                    if not any(skip_section in section_title for skip_section in skip_sections):
                        content = self.extract_section_content(soup, section_id)
                        if content:
                            all_sections[section_id] = {
                                'title': section_title,
                                'content': content
                            }
                enemy_data['sections'] = all_sections
            
            return enemy_data
            
        except Exception as e:
            print(f"è§£ææ•Œäººé¡µé¢å¤±è´¥: {e}")
            return None

    def _is_enemy_page(self, soup: BeautifulSoup, title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•Œäººé¡µé¢"""
        # æ£€æŸ¥é¡µé¢æ ‡é¢˜å’Œå†…å®¹ç‰¹å¾
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰"æ•Œäººä¸€è§ˆ"é“¾æ¥
        enemy_overview_link = soup.find('a', href='/w/æ•Œäººä¸€è§ˆ')
        if enemy_overview_link:
            return True
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ•Œäººç‰¹æœ‰çš„ç« èŠ‚
        enemy_sections = ['çº§åˆ«0', 'çº§åˆ«1', 'çº§åˆ«2', 'æ•Œäººæ¨¡å‹']
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            heading_text = heading.get_text(strip=True)
            if any(section in heading_text for section in enemy_sections):
                return True
        
        # 3. æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰æ•Œäººç‰¹æœ‰å†…å®¹
        toc_div = soup.find('div', {'id': 'toc'}) or soup.find('div', class_='toc')
        if toc_div:
            toc_text = toc_div.get_text()
            if any(section in toc_text for section in enemy_sections):
                return True
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰æ•Œäººæ•°æ®è¡¨æ ¼
        for table in soup.find_all('table'):
            table_text = table.get_text()
            if 'ç”Ÿå‘½å€¼' in table_text and 'æ”»å‡»åŠ›' in table_text and 'é˜²å¾¡åŠ›' in table_text:
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æœ‰æ•Œäººç‰¹æœ‰çš„å±æ€§
                if any(attr in table_text for attr in ['ç§»åŠ¨é€Ÿåº¦', 'æ”»å‡»é—´éš”', 'é‡é‡', 'é˜»æŒ¡æ•°']):
                    return True
        
        return False

    def _extract_enemy_basic_info(self, soup: BeautifulSoup, enemy_data: Dict):
        """æå–æ•ŒäººåŸºæœ¬ä¿¡æ¯"""
        basic_info = {}
        
        # ä»é¡µé¢ä¸­æå–æ•Œäººçš„åŸºæœ¬å±æ€§
        for table in soup.find_all('table'):
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['th', 'td'])
                if len(cells) >= 2:
                    header = self.extract_text_from_cell(cells[0])
                    value = self.extract_text_from_cell(cells[1])
                    
                    # è¯†åˆ«å¸¸è§çš„æ•Œäººå±æ€§
                    if any(keyword in header for keyword in ['åˆ†ç±»', 'ç§æ—', 'é‡é‡', 'é˜»æŒ¡æ•°']):
                        basic_info[header] = value
        
        enemy_data['basic_info'] = basic_info

    def _extract_enemy_levels(self, soup: BeautifulSoup, enemy_data: Dict):
        """æå–æ•Œäººç­‰çº§ä¿¡æ¯"""
        levels = {}
        
        # æŸ¥æ‰¾çº§åˆ«ç« èŠ‚
        for heading in soup.find_all(['h2', 'h3']):
            heading_text = heading.get_text(strip=True)
            if 'çº§åˆ«' in heading_text:
                level_id = heading.get('id', heading_text)
                
                # æŸ¥æ‰¾è¯¥çº§åˆ«ä¸‹çš„æ•°æ®è¡¨æ ¼
                level_data = self._extract_enemy_level_data(heading)
                if level_data:
                    levels[level_id] = {
                        'title': heading_text,
                        'data': level_data
                    }
        
        enemy_data['levels'] = levels

    def _extract_enemy_level_data(self, level_heading) -> List[Dict]:
        """æå–æŒ‡å®šçº§åˆ«çš„æ•Œäººæ•°æ®"""
        level_data = []
        
        # ä»çº§åˆ«æ ‡é¢˜å¼€å§‹ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ªè¡¨æ ¼
        current_element = level_heading.next_sibling
        while current_element:
            if hasattr(current_element, 'name'):
                if current_element.name == 'table':
                    # è§£ææ•Œäººæ•°æ®è¡¨æ ¼
                    table_data = self._parse_enemy_table(current_element)
                    if table_data:
                        level_data.extend(table_data)
                    break
                elif current_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    # é‡åˆ°æ–°çš„æ ‡é¢˜ï¼Œåœæ­¢æœç´¢
                    break
            
            current_element = current_element.next_sibling
        
        return level_data

    def _parse_enemy_table(self, table) -> List[Dict]:
        """è§£ææ•Œäººæ•°æ®è¡¨æ ¼"""
        enemies = []
        rows = table.find_all('tr')
        
        if not rows:
            return enemies
        
        # è·å–è¡¨å¤´
        header_row = rows[0]
        headers = [self.extract_text_from_cell(cell) for cell in header_row.find_all(['th', 'td'])]
        
        # è§£ææ•°æ®è¡Œ
        for row in rows[1:]:
            cells = row.find_all(['td', 'th'])
            if len(cells) >= len(headers):
                enemy_data = {}
                for i, cell in enumerate(cells[:len(headers)]):
                    if i < len(headers):
                        header = headers[i]
                        value = self.extract_text_from_cell(cell)
                        if header and value:
                            enemy_data[header] = value
                
                if enemy_data:
                    enemies.append(enemy_data)
        
        return enemies

    def _format_enemy_info(self, enemy_data: Dict, target_sections: Optional[List[str]] = None) -> str:
        """æ ¼å¼åŒ–æ•Œäººä¿¡æ¯è¾“å‡º"""
        if not enemy_data:
            return "æ•Œäººä¿¡æ¯è§£æå¤±è´¥"
        
        result = [f"# {enemy_data.get('title', 'æœªçŸ¥æ•Œäºº')}"]
        
        # åŸºæœ¬ä¿¡æ¯
        if enemy_data.get('basic_info'):
            result.append("\n## ğŸ“‹ åŸºæœ¬ä¿¡æ¯")
            for key, value in enemy_data['basic_info'].items():
                result.append(f"- **{key}**: {value}")
        
        # ç­‰çº§ä¿¡æ¯
        if enemy_data.get('levels'):
            result.append("\n## ğŸ“Š ç­‰çº§æ•°æ®")
            for level_id, level_info in enemy_data['levels'].items():
                result.append(f"\n### {level_info['title']}")
                if level_info.get('data'):
                    for enemy in level_info['data']:
                        result.append(f"\n**{enemy.get('åç§°', 'æœªçŸ¥æ•Œäºº')}**:")
                        for attr, value in enemy.items():
                            if attr != 'åç§°':
                                result.append(f"- {attr}: {value}")
        
        # é¡µé¢ç›®å½•ï¼ˆä»…åœ¨å®Œæ•´æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
        if not target_sections and enemy_data.get('table_of_contents'):
            result.append("\n## ğŸ“š é¡µé¢ç›®å½•")
            for section_id, toc_info in enemy_data['table_of_contents'].items():
                indent = "  " * (toc_info.get('level', 1) - 1)
                result.append(f"{indent}- {toc_info['title']}")
        
        # ç« èŠ‚å†…å®¹
        if enemy_data.get('sections'):
            for section_id, section_info in enemy_data['sections'].items():
                result.append(f"\n## {section_info['title']}")
                result.append(section_info['content'])
        
        return '\n'.join(result)

    async def _verify_operator_page(self, title: str) -> bool:
        """éªŒè¯é¡µé¢æ˜¯å¦çœŸçš„æ˜¯å¹²å‘˜é¡µé¢ï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰"å¹²å‘˜ä¿¡æ¯"æ ç›®ï¼‰"""
        try:
            html = await self.get_page_html(title)
            if not html:
                return False
            
            soup = BeautifulSoup(html, 'lxml')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"å¹²å‘˜ä¿¡æ¯"ç›¸å…³çš„æ ‡é¢˜æˆ–å†…å®¹
            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                heading_text = heading.get_text(strip=True)
                if 'å¹²å‘˜ä¿¡æ¯' in heading_text:
                    return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰èŒä¸šã€ç¨€æœ‰åº¦ç­‰å¹²å‘˜ç‰¹æœ‰ä¿¡æ¯
            page_text = soup.get_text()
            if any(indicator in page_text for indicator in ['â˜…â˜…â˜…â˜…â˜…â˜…', 'â˜…â˜…â˜…â˜…â˜…', 'â˜…â˜…â˜…â˜…', 'â˜…â˜…â˜…', 'ç²¾è‹±åŒ–', 'æ½œèƒ½æå‡']):
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰èŒä¸šæ ‡è¯†
            if any(prof in page_text for prof in ['åŒ»ç–—å¹²å‘˜', 'æœ¯å¸ˆå¹²å‘˜', 'ç‹™å‡»å¹²å‘˜', 'é‡è£…å¹²å‘˜', 'è¿‘å«å¹²å‘˜', 'å…ˆé”‹å¹²å‘˜', 'è¾…åŠ©å¹²å‘˜', 'ç‰¹ç§å¹²å‘˜']):
                return True
                
            return False
            
        except Exception as e:
            print(f"éªŒè¯å¹²å‘˜é¡µé¢å¤±è´¥ {title}: {e}")
            return False
    
    async def _verify_enemy_page(self, title: str) -> bool:
        """éªŒè¯é¡µé¢æ˜¯å¦çœŸçš„æ˜¯æ•Œäººé¡µé¢ï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰"æ•Œäººæ¨¡å‹"æˆ–çº§åˆ«ä¿¡æ¯ï¼‰"""
        try:
            html = await self.get_page_html(title)
            if not html:
                return False
            
            soup = BeautifulSoup(html, 'lxml')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"æ•Œäººæ¨¡å‹"æ ç›®
            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                heading_text = heading.get_text(strip=True)
                if 'æ•Œäººæ¨¡å‹' in heading_text:
                    return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰çº§åˆ«ä¿¡æ¯
            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                heading_text = heading.get_text(strip=True)
                if any(level in heading_text for level in ['çº§åˆ«0', 'çº§åˆ«1', 'çº§åˆ«2']):
                    return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•Œäººç‰¹æœ‰çš„å±æ€§è¡¨æ ¼
            page_text = soup.get_text()
            if 'ç§»åŠ¨é€Ÿåº¦' in page_text and 'æ”»å‡»é—´éš”' in page_text and 'é‡é‡' in page_text:
                return True
                
            return False
            
        except Exception as e:
            print(f"éªŒè¯æ•Œäººé¡µé¢å¤±è´¥ {title}: {e}")
            return False


