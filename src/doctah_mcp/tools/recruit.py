#!/usr/bin/env python3
"""
å…¬æ‹›è®¡ç®—ï¼ˆå…¬å¼€æ‹›å‹Ÿæ ‡ç­¾åŒ¹é…ï¼‰

å‚è€ƒ PRTSã€å…¬æ‹›è®¡ç®—ã€é¡µé¢çš„æ•°æ®æ¥æºï¼Œé€šè¿‡ cargoquery æ‹‰å–å¯å…¬æ‹›å¹²å‘˜ä¸å…¶æ ‡ç­¾ï¼Œ
æ ¹æ®ç»™å®šçš„ç­›é€‰è¯æ¡ï¼ˆèµ„å†/èŒä¸š/ä½ç½®/è¯ç¼€ï¼‰è¿”å›å¯èƒ½å‡ºç°çš„å¹²å‘˜åˆ—è¡¨ã€‚

æ•°æ®æ¥æºç¤ºä¾‹ï¼ˆä¸å‰ç«¯ä¸€è‡´ï¼‰ï¼š
- /api.php?action=cargoquery&format=json&tables=chara,char_obtain&limit=5000
  &fields=chara.profession,chara.position,chara.rarity,chara.tag,chara.cn,char_obtain.obtainMethod
  &where=char_obtain.obtainMethod like "%å…¬å¼€ æ‹›å‹Ÿ%" AND chara.charIndex>0
  &join_on=chara._pageName=char_obtain._pageName
"""

import asyncio
from typing import Dict, List, Optional
from urllib.parse import quote

from ..client import PRTSWikiClient
from .utils import BASE_URL

# å¸¸é‡ï¼ˆä¸é¡µé¢ä¸€è‡´ï¼‰
PROFESSIONS = ["è¿‘å«", "ç‹™å‡»", "é‡è£…", "åŒ»ç–—", "è¾…åŠ©", "æœ¯å¸ˆ", "ç‰¹ç§", "å…ˆé”‹"]
POSITIONS = ["è¿‘æˆ˜ä½", "è¿œç¨‹ä½"]
RARITY_TAGS = ["é«˜çº§èµ„æ·±å¹²å‘˜", "èµ„æ·±å¹²å‘˜", "æ–°æ‰‹"]  # æ³¨æ„ï¼š"æ–°æ‰‹"åœ¨ chara.tag å­—æ®µä¸­å‡ºç°
TAG_WORDS = [
    "æ”¯æ´æœºæ¢°", "æ§åœº", "çˆ†å‘", "æ²»ç–—", "æ”¯æ´", "è´¹ç”¨å›å¤", "è¾“å‡º", "ç”Ÿå­˜",
    "ç¾¤æ”»", "é˜²æŠ¤", "å‡é€Ÿ", "å‰Šå¼±", "å¿«é€Ÿå¤æ´»", "ä½ç§»", "å¬å”¤", "å…ƒç´ ",
]

# åˆ«åå½’ä¸€
PROF_ALIASES = {
    "è¿‘å«å¹²å‘˜": "è¿‘å«", "ç‹™å‡»å¹²å‘˜": "ç‹™å‡»", "é‡è£…å¹²å‘˜": "é‡è£…", "åŒ»ç–—å¹²å‘˜": "åŒ»ç–—",
    "è¾…åŠ©å¹²å‘˜": "è¾…åŠ©", "æœ¯å¸ˆå¹²å‘˜": "æœ¯å¸ˆ", "ç‰¹ç§å¹²å‘˜": "ç‰¹ç§", "å…ˆé”‹å¹²å‘˜": "å…ˆé”‹",
}
POS_ALIASES = {"è¿‘æˆ˜": "è¿‘æˆ˜ä½", "è¿œç¨‹": "è¿œç¨‹ä½", "è¿‘æˆ˜ä½": "è¿‘æˆ˜ä½", "è¿œç¨‹ä½": "è¿œç¨‹ä½"}
RARITY_ALIASES = {"é«˜èµ„": "é«˜çº§èµ„æ·±å¹²å‘˜", "é«˜èµ„æ·±": "é«˜çº§èµ„æ·±å¹²å‘˜", "èµ„æ·±": "èµ„æ·±å¹²å‘˜", "èµ„æ·±å¹²å‘˜": "èµ„æ·±å¹²å‘˜", "æ–°æ‰‹å¹²å‘˜": "æ–°æ‰‹"}
TAG_ALIASES = {"è´¹ç”¨å›æ”¶": "è´¹ç”¨å›å¤"}


def _split_terms(terms: Optional[str]) -> List[str]:
    if not terms:
        return []
    text = terms
    for sep in [",", "ï¼Œ", "ã€", "|", " "]:
        text = text.replace(sep, ",")
    tokens = [t.strip() for t in text.split(",") if t.strip()]
    # å½’ä¸€æ˜ å°„
    normed = []
    for t in tokens:
        t2 = RARITY_ALIASES.get(t, PROF_ALIASES.get(t, POS_ALIASES.get(t, TAG_ALIASES.get(t, t))))
        # å»æ‰â€œå¹²å‘˜â€åç¼€ï¼ˆå…œåº•ï¼‰
        if t2.endswith("å¹²å‘˜"):
            t2 = t2[:-2]
        normed.append(t2)
    return normed


def _classify_terms(terms: List[str]):
    """å°†è¯æ¡æŒ‰ç±»åˆ«æ‹†åˆ†ï¼šèŒä¸š/ä½ç½®/èµ„å†tag/è¯ç¼€ã€‚"""
    pros, poss, rars, tags = set(), set(), set(), []
    for t in terms:
        if t in PROFESSIONS:
            pros.add(t)
        elif t in POSITIONS or t in POS_ALIASES.values():
            poss.add(POS_ALIASES.get(t, t))
        elif t in RARITY_TAGS or t in RARITY_ALIASES.values():
            rars.add(RARITY_ALIASES.get(t, t))
        elif t in TAG_WORDS or t in TAG_ALIASES.values():
            tags.append(TAG_ALIASES.get(t, t))
        else:
            # æœªè¯†åˆ«è¯æ¡å°è¯•å»æ‰â€œå¹²å‘˜â€åç¼€å†åŒ¹é…
            if t.endswith("å¹²å‘˜") and t[:-2] in PROFESSIONS:
                pros.add(t[:-2])
            else:
                tags.append(t)  # å½“ä½œæ™®é€šè¯ç¼€å°è¯•
    return pros, poss, rars, tags


async def recruit_by_tags(terms: str, wiki_client: Optional[PRTSWikiClient] = None) -> str:
    """
    ç»™å®šè¯æ¡ï¼ˆä»¥é€—å·/é¡¿å·/ç©ºæ ¼/ç«–çº¿åˆ†éš”ï¼‰ï¼Œè¿”å›å¯èƒ½å‡ºç°çš„å…¬æ‹›å¹²å‘˜ã€‚
    æ”¯æŒç»´åº¦ï¼šèµ„å†(é«˜çº§èµ„æ·±å¹²å‘˜/èµ„æ·±å¹²å‘˜/æ–°æ‰‹)ã€èŒä¸šã€ä½ç½®ã€è¯ç¼€ï¼ˆå¦‚ è¾“å‡º/æ²»ç–—/è´¹ç”¨å›å¤ ç­‰ï¼‰ã€‚
    åŒä¸€ç±»åˆ«å†…ä¸ºâ€œæˆ–â€åŒ¹é…ï¼ˆèŒä¸š/ä½ç½®/èµ„å†ï¼‰ï¼Œè¯ç¼€ä¸ºâ€œä¸”â€åŒ¹é…ã€‚
    """
    selected = _split_terms(terms)
    if not selected:
        return (
            "# âŒ å…¬æ‹›è®¡ç®—å¤±è´¥\n\n"
            "- **çŠ¶æ€**: ç¼ºå°‘ç­›é€‰è¯æ¡\n"
            "- **ç¤ºä¾‹**: èµ„æ·±å¹²å‘˜, æœ¯å¸ˆ, ç¾¤æ”» æˆ– å…ˆé”‹ è´¹ç”¨å›å¤\n"
        )

    pros, poss, rars, tag_need = _classify_terms(selected)

    client_provided = wiki_client is not None
    if not wiki_client:
        wiki_client = PRTSWikiClient()

    try:
        # æ‹‰å–å¯å…¬å¼€æ‹›å‹Ÿçš„å¹²å‘˜æ•°æ®
        params = {
            "action": "cargoquery",
            "format": "json",
            "tables": "chara,char_obtain",
            "limit": "5000",
            "fields": "chara.profession,chara.position,chara.rarity,chara.tag,chara.cn,char_obtain.obtainMethod",
            "where": 'char_obtain.obtainMethod like "%å…¬å¼€%æ‹›å‹Ÿ%" AND chara.charIndex>0',
            "join_on": "chara._pageName=char_obtain._pageName",
        }
        resp = await wiki_client.session.get(f"{BASE_URL}/api.php", params=params)
        resp.raise_for_status()
        data = resp.json().get("cargoquery", [])

        candidates: List[Dict] = []
        for item in data:
            title = item.get("title", {})
            name = title.get("cn")
            if not name:
                continue
            rarity_num = int(title.get("rarity", "0") or 0)  # 0..5
            tags = (title.get("tag") or "").split(" ") if title.get("tag") else []
            obtain = (title.get("obtainMethod") or "").split(" ") if title.get("obtainMethod") else []
            # èµ„å†æ ‡ç­¾
            rarity_tag = "é«˜çº§èµ„æ·±å¹²å‘˜" if rarity_num == 5 else ("èµ„æ·±å¹²å‘˜" if rarity_num == 4 else None)

            # OR åŒ¹é…ï¼šèŒä¸š/ä½ç½®/èµ„å†
            if pros and (title.get("profession") not in pros):
                continue
            if poss and (title.get("position") not in poss):
                continue
            if rars and (rarity_tag not in rars):
                continue
            # AND åŒ¹é…ï¼šè¯ç¼€
            if tag_need and not set(tag_need).issubset(set(tags)):
                continue

            candidates.append({
                "name": name,
                "rarity": rarity_num + 1,
                "profession": title.get("profession", ""),
                "position": title.get("position", ""),
                "tags": tags,
                "obtain": obtain,
                "rarity_tag": rarity_tag or "",
                "hit_tags": [t for t in tag_need if t in tags],
                "url": f"{BASE_URL}/w/{quote(name)}",
            })

        if not candidates:
            # è¿”å›è¯Šæ–­ä¿¡æ¯
            diag = []
            if pros: diag.append(f"èŒä¸šâˆˆ{','.join(pros)}")
            if poss: diag.append(f"ä½ç½®âˆˆ{','.join(poss)}")
            if rars: diag.append(f"èµ„å†âˆˆ{','.join(rars)}")
            if tag_need: diag.append(f"è¯ç¼€âŠ‡{','.join(tag_need)}")
            return (
                "# ğŸ” å…¬æ‹›è®¡ç®—\n\n"
                f"- **åŒ¹é…æ•°é‡**: 0\n- **è¯æ¡**: {', '.join(selected)}\n"
                f"- **ç­›é€‰æ¡ä»¶**: {'ï¼›'.join(diag) if diag else 'æ— '}\n"
                "- å¯èƒ½åŸå› ï¼š\n  1) é€‰æ‹©äº†äº’æ–¥èŒä¸šï¼›2) è¯ç¼€ç»„åˆè¿‡äºè‹›åˆ»ï¼ˆå¦‚â€œå…ƒç´ â€å½“å‰å…¬å¼€æ‹›å‹Ÿä¸­åŸºæœ¬ä¸å­˜åœ¨ï¼‰ï¼›3) è¯æ¡æ‹¼å†™æˆ–åˆ«åä¸ä¸€è‡´ã€‚\n"
            )

        candidates.sort(key=lambda x: (-x["rarity"], x["profession"], x["name"]))
        groups: Dict[int, List[Dict]] = {}
        for c in candidates:
            groups.setdefault(c["rarity"], []).append(c)

        lines = [
            "# ğŸ¯ å…¬æ‹›è®¡ç®—ç»“æœ",
            "",
            f"- **è¯æ¡**: {', '.join(selected)}",
            f"- **åŒ¹é…å¹²å‘˜**: {len(candidates)}",
            "",
        ]
        for rarity in sorted(groups.keys(), reverse=True):
            lines.append(f"## {rarity}â˜…")
            for op in groups[rarity]:
                meta = [op["profession"], op["position"], " ".join(op["tags"])[:60]]
                meta = [m for m in meta if m]
                lines.append(f"- **{op['name']}**ï¼ˆ{' / '.join(meta)}ï¼‰")
                # åŒ¹é…ä¾æ®
                reason = []
                if op.get("profession"): reason.append(f"èŒä¸š={op['profession']}")
                if op.get("position"): reason.append(f"ä½ç½®={op['position']}")
                if op.get("rarity_tag"): reason.append(f"èµ„å†={op['rarity_tag']}")
                if op.get("hit_tags"): reason.append(f"è¯ç¼€å‘½ä¸­={','.join(op['hit_tags'])}")
                if reason:
                    lines.append(f"  åŒ¹é…ä¾æ®: {'ï¼›'.join(reason)}")
                lines.append(f"  é“¾æ¥: {op['url']}")
            lines.append("")

        lines.append("---")
        lines.append("æ•°æ®æ¥æº: å…¬æ‹›è®¡ç®— / cargoquery")
        return "\n".join(lines)

    except Exception as e:
        return f"# âŒ å…¬æ‹›è®¡ç®—é”™è¯¯\n\n- **é”™è¯¯ä¿¡æ¯**: {e}"
    finally:
        if not client_provided:
            await wiki_client.close()


def _build_universe():
    """è¿”å›ä¸å‰ç«¯ä¸€è‡´çš„å¹¶é›†é¡ºåºä¸ç´¢å¼•æ˜ å°„ã€‚"""
    R = PROFESSIONS + POSITIONS + RARITY_TAGS + TAG_WORDS
    idx = {name: i for i, name in enumerate(R)}
    return R, idx


def _bits_set(indices: List[int]) -> int:
    v = 0
    for i in indices:
        v |= (1 << i)
    return v


def _subset_bitsets(bitmask: int) -> List[int]:
    """è¿”å› bitmask çš„æ‰€æœ‰éç©ºå­é›†ä½å›¾ã€‚"""
    bits = []
    i = 0
    while (1 << i) <= bitmask:
        if bitmask & (1 << i):
            bits.append(i)
        i += 1
    res = []
    total = 1 << len(bits)
    for s in range(1, total):
        v = 0
        for j, bi in enumerate(bits):
            if s & (1 << j):
                v |= (1 << bi)
        res.append(v)
    return res


async def recruit_by_tags_all(terms: str, wiki_client: Optional[PRTSWikiClient] = None) -> str:
    """
    ä¸¥æ ¼ä½¿ç”¨æ‰€æœ‰è¯æ¡ï¼ˆANDï¼‰ï¼šåªæœ‰å½“å¹²å‘˜å…·å¤‡â€œæ‰€æœ‰ç»™å®šè¯æ¡â€æ—¶æ‰ä¼šå‘½ä¸­ã€‚
    è¯æ¡ç»Ÿä¸€ä½¿ç”¨èŒä¸š/ä½ç½®/èµ„å†/è¯ç¼€å…¨é›†ï¼Œæ— â€œæˆ–â€é€»è¾‘ã€‚
    """
    selected = _split_terms(terms)
    if not selected:
        return "# âŒ å…¬æ‹›è®¡ç®—ï¼ˆä¸¥æ ¼ï¼‰\n\n- **çŠ¶æ€**: ç¼ºå°‘è¯æ¡"
    R, IDX = _build_universe()
    need_bits = 0
    for t in selected:
        if t in IDX:
            need_bits |= (1 << IDX[t])
    client_provided = wiki_client is not None
    if not wiki_client:
        wiki_client = PRTSWikiClient()
    try:
        params = {
            "action": "cargoquery",
            "format": "json",
            "tables": "chara,char_obtain",
            "limit": "5000",
            "fields": "chara.profession,chara.position,chara.rarity,chara.tag,chara.cn,char_obtain.obtainMethod",
            "where": 'char_obtain.obtainMethod like "%å…¬å¼€%æ‹›å‹Ÿ%" AND chara.charIndex>0',
            "join_on": "chara._pageName=char_obtain._pageName",
        }
        resp = await wiki_client.session.get(f"{BASE_URL}/api.php", params=params)
        resp.raise_for_status()
        data = resp.json().get("cargoquery", [])

        hits = []
        for item in data:
            t = item.get("title", {})
            name = t.get("cn")
            if not name:
                continue
            star = int(t.get("rarity", 0)) + 1
            profession = t.get("profession", "")
            position = t.get("position", "")
            tags = (t.get("tag") or "").split(" ") if t.get("tag") else []
            rarity_tag = "é«˜çº§èµ„æ·±å¹²å‘˜" if star == 6 else ("èµ„æ·±å¹²å‘˜" if star == 5 else None)
            op_bits = 0
            if profession in IDX: op_bits |= (1 << IDX[profession])
            if position in IDX: op_bits |= (1 << IDX[position])
            if rarity_tag and rarity_tag in IDX: op_bits |= (1 << IDX[rarity_tag])
            for tg in tags:
                if tg in IDX: op_bits |= (1 << IDX[tg])
            if (op_bits & need_bits) == need_bits:
                hits.append({
                    "name": name,
                    "star": star,
                    "profession": profession,
                    "position": position,
                    "tags": tags,
                    "url": f"{BASE_URL}/w/{quote(name)}",
                })

        if not hits:
            return (
                "# ğŸ” å…¬æ‹›è®¡ç®—ï¼ˆä¸¥æ ¼ï¼‰\n\n"
                f"- **è¯æ¡**: {', '.join(selected)}\n- **åŒ¹é…æ•°é‡**: 0\n"
                "- æç¤ºï¼šè‹¥åŒ…å«äº’æ–¥èŒä¸š/ä½ç½®ï¼Œå°†å¯¼è‡´å¿…ç„¶ä¸ºç©ºã€‚\n"
            )

        hits.sort(key=lambda x: (-x["star"], x["profession"], x["name"]))
        lines = ["# ğŸ¯ å…¬æ‹›è®¡ç®—ç»“æœï¼ˆä¸¥æ ¼ä½¿ç”¨å…¨éƒ¨è¯æ¡ï¼‰", "", f"- **è¯æ¡**: {', '.join(selected)}", f"- **åŒ¹é…å¹²å‘˜**: {len(hits)}", ""]
        for h in hits:
            meta = [f"{h['star']}â˜…", h['profession'], h['position']]
            lines.append(f"- **{h['name']}**ï¼ˆ{' / '.join([m for m in meta if m])}ï¼‰")
            lines.append(f"  é“¾æ¥: {h['url']}")
        return "\n".join(lines)
    except Exception as e:
        return f"# âŒ å…¬æ‹›è®¡ç®—ï¼ˆä¸¥æ ¼ï¼‰é”™è¯¯\n\n- **é”™è¯¯ä¿¡æ¯**: {e}"
    finally:
        if not client_provided:
            await wiki_client.close()


async def recruit_by_tags_suggest(terms: str, top_k: int = 10, wiki_client: Optional[PRTSWikiClient] = None) -> str:
    """
    è‡ªåŠ¨å¯»æ‰¾â€œèƒ½å‡ºç»“æœâ€çš„æœ€ä½³å­é›†ç»„åˆï¼ˆä¸è¦æ±‚ç”¨å®Œæ‰€æœ‰è¯æ¡ï¼‰ã€‚
    - è¯„åˆ†ï¼šå…ˆæŒ‰å¹³å‡æ˜Ÿçº§ï¼Œå†æŒ‰å‘½ä¸­æ•°é‡æ’åºï¼Œå–å‰ top_k ä¸ªç»„åˆã€‚
    - 6â˜… ç»„åˆä»…åœ¨åŒ…å«â€œé«˜çº§èµ„æ·±å¹²å‘˜â€æ—¶æ˜¾ç¤ºã€‚
    """
    selected = _split_terms(terms)
    if not selected:
        return "# âŒ å…¬æ‹›è®¡ç®—ï¼ˆå»ºè®®ç»„åˆï¼‰\n\n- **çŠ¶æ€**: ç¼ºå°‘è¯æ¡"
    R, IDX = _build_universe()
    sel_bits = 0
    for t in selected:
        if t in IDX:
            sel_bits |= (1 << IDX[t])
    client_provided = wiki_client is not None
    if not wiki_client:
        wiki_client = PRTSWikiClient()
    try:
        params = {
            "action": "cargoquery",
            "format": "json",
            "tables": "chara,char_obtain",
            "limit": "5000",
            "fields": "chara.profession,chara.position,chara.rarity,chara.tag,chara.cn,char_obtain.obtainMethod",
            "where": 'char_obtain.obtainMethod like "%å…¬å¼€%æ‹›å‹Ÿ%" AND chara.charIndex>0',
            "join_on": "chara._pageName=char_obtain._pageName",
        }
        resp = await wiki_client.session.get(f"{BASE_URL}/api.php", params=params)
        resp.raise_for_status()
        data = resp.json().get("cargoquery", [])

        groups: Dict[int, Dict] = {}
        HIGH = IDX.get("é«˜çº§èµ„æ·±å¹²å‘˜", -1)
        for item in data:
            t = item.get("title", {})
            name = t.get("cn")
            if not name:
                continue
            star = int(t.get("rarity", 0)) + 1
            profession = t.get("profession", "")
            position = t.get("position", "")
            tags = (t.get("tag") or "").split(" ") if t.get("tag") else []
            rarity_tag = "é«˜çº§èµ„æ·±å¹²å‘˜" if star == 6 else ("èµ„æ·±å¹²å‘˜" if star == 5 else None)
            op_bits = 0
            if profession in IDX: op_bits |= (1 << IDX[profession])
            if position in IDX: op_bits |= (1 << IDX[position])
            if rarity_tag and rarity_tag in IDX: op_bits |= (1 << IDX[rarity_tag])
            for tg in tags:
                if tg in IDX: op_bits |= (1 << IDX[tg])
            for subset in _subset_bitsets(op_bits):
                if (sel_bits | subset) != sel_bits:
                    continue
                if star == 6 and HIGH >= 0 and not (subset & (1 << HIGH)):
                    continue
                g = groups.setdefault(subset, {"ops": [], "stars": []})
                g["ops"].append({"name": name, "star": star, "profession": profession, "position": position, "url": f"{BASE_URL}/w/{quote(name)}"})
                g["stars"].append(star)

        if not groups:
            return (
                "# ğŸ” å…¬æ‹›è®¡ç®—ï¼ˆå»ºè®®ç»„åˆï¼‰\n\n"
                f"- **è¯æ¡**: {', '.join(selected)}\n- æ²¡æœ‰èƒ½äº§ç”Ÿç»“æœçš„ç»„åˆ\n"
            )

        def score(item):
            subset, g = item
            avg = sum(g["stars"]) / max(1, len(g["stars"]))
            return (avg, len(g["ops"]))
        ordered = sorted(groups.items(), key=score, reverse=True)[:top_k]

        def names(subset: int) -> List[str]:
            out = []
            i = 0
            while (1 << i) <= subset:
                if subset & (1 << i):
                    out.append(R[i])
                i += 1
            return out

        lines = ["# ğŸ¯ å…¬æ‹›è®¡ç®—å»ºè®®ç»„åˆ", "", f"- **è¯æ¡**: {', '.join(selected)}", f"- **ç»„åˆæ•°**: {len(ordered)}", ""]
        for subset, g in ordered:
            title = "+".join(names(subset))
            avg = sum(g["stars"]) / max(1, len(g["stars"]))
            lines.append(f"## {title}  â€” å¹³å‡æ˜Ÿçº§â‰ˆ{avg:.2f}ï¼Œäººæ•°={len(g['ops'])}")
            for op in sorted(g["ops"], key=lambda x: (-x["star"], x["profession"], x["name"])):
                lines.append(f"- **{op['name']}**ï¼ˆ{op['star']}â˜… / {op['profession']} / {op['position']}ï¼‰")
                lines.append(f"  é“¾æ¥: {op['url']}")
            lines.append("")
        lines.append("---")
        lines.append("æ•°æ®æ¥æº: å…¬æ‹›è®¡ç®— / cargoqueryï¼ˆå»ºè®®ç»„åˆï¼‰")
        return "\n".join(lines)
    except Exception as e:
        return f"# âŒ å…¬æ‹›è®¡ç®—ï¼ˆå»ºè®®ç»„åˆï¼‰é”™è¯¯\n\n- **é”™è¯¯ä¿¡æ¯**: {e}"
    finally:
        if not client_provided:
            await wiki_client.close()


async def recruit_by_tags_grouped(terms: str, wiki_client: Optional[PRTSWikiClient] = None) -> str:
    """
    æŒ‰ç½‘é¡µé€»è¾‘å°†ç»“æœæŒ‰â€œæ‰€é€‰è¯æ¡çš„å­é›†ç»„åˆâ€è¿›è¡Œåˆ†ç»„å±•ç¤ºã€‚

    - è¾“å…¥ï¼šåŒ recruit_by_tags çš„ terms
    - åˆ†ç»„ï¼šå°†æ¯ä½å¹²å‘˜çš„(èŒä¸š/ä½ç½®/èµ„å†/è¯ç¼€)å½¢æˆä½å›¾ï¼Œæšä¸¾å…¶æ‰€æœ‰éç©ºå­é›†ï¼›
      è‹¥è¯¥å­é›†å®Œå…¨åŒ…å«åœ¨â€œæ‰€é€‰è¯æ¡ä½å›¾â€å†…ï¼Œåˆ™å°†è¯¥å¹²å‘˜è®¡å…¥è¯¥å­é›†çš„åˆ†ç»„ã€‚
      å¯¹äº 6â˜… å¹²å‘˜ï¼Œä»…å½“å­é›†åŒ…å«â€œé«˜çº§èµ„æ·±å¹²å‘˜â€æ—¶æ‰è®¡å…¥ï¼ˆä¸å‰ç«¯ä¸€è‡´ï¼‰ã€‚
    - æ’åºï¼šæŒ‰å„ç»„å¹³å‡æ˜Ÿçº§ç”±é«˜åˆ°ä½ï¼Œå†æŒ‰ç»„å†…äººæ•°ç”±å¤šåˆ°å°‘ã€‚
    """
    selected = _split_terms(terms)
    pros, poss, rars, tag_need = _classify_terms(selected)
    R, IDX = _build_universe()

    # æ‰€é€‰è¯æ¡ä½å›¾
    selected_bits = 0
    for t in selected:
        if t in IDX:
            selected_bits |= (1 << IDX[t])

    client_provided = wiki_client is not None
    if not wiki_client:
        wiki_client = PRTSWikiClient()

    try:
        params = {
            "action": "cargoquery",
            "format": "json",
            "tables": "chara,char_obtain",
            "limit": "5000",
            "fields": "chara.profession,chara.position,chara.rarity,chara.tag,chara.cn,char_obtain.obtainMethod",
            "where": 'char_obtain.obtainMethod like "%å…¬å¼€%æ‹›å‹Ÿ%" AND chara.charIndex>0',
            "join_on": "chara._pageName=char_obtain._pageName",
        }
        resp = await wiki_client.session.get(f"{BASE_URL}/api.php", params=params)
        resp.raise_for_status()
        data = resp.json().get("cargoquery", [])

        groups: Dict[int, Dict] = {}
        def star_from_rarity(rn: int) -> int:
            return int(rn) + 1

        HIGH_TAG_BIT = IDX.get("é«˜çº§èµ„æ·±å¹²å‘˜", -1)

        for item in data:
            t = item.get("title", {})
            name = t.get("cn")
            if not name:
                continue
            rarity_num = int(t.get("rarity", "0") or 0)
            star = star_from_rarity(rarity_num)
            profession = t.get("profession", "")
            position = t.get("position", "")
            tags = (t.get("tag") or "").split(" ") if t.get("tag") else []
            rarity_tag = "é«˜çº§èµ„æ·±å¹²å‘˜" if star == 6 else ("èµ„æ·±å¹²å‘˜" if star == 5 else None)

            # æ„å»ºå¹²å‘˜ä½å›¾
            op_bits = 0
            if profession in IDX: op_bits |= (1 << IDX[profession])
            if position in IDX: op_bits |= (1 << IDX[position])
            if rarity_tag and rarity_tag in IDX: op_bits |= (1 << IDX[rarity_tag])
            for tg in tags:
                if tg in IDX:
                    op_bits |= (1 << IDX[tg])

            # ç”Ÿæˆæ‰€æœ‰éç©ºå­é›†
            for subset in _subset_bitsets(op_bits):
                # å­é›†éœ€å®Œå…¨åŒ…å«åœ¨æ‰€é€‰è¯æ¡ä¸­
                if (selected_bits | subset) != selected_bits:
                    continue
                # 6â˜… ä»…å½“å­é›†åŒ…å« é«˜çº§èµ„æ·±å¹²å‘˜
                if star == 6 and HIGH_TAG_BIT is not None and HIGH_TAG_BIT >= 0:
                    if not (subset & (1 << HIGH_TAG_BIT)):
                        continue
                # è®°å½•
                g = groups.setdefault(subset, {"ops": [], "stars": []})
                g["ops"].append({
                    "name": name,
                    "star": star,
                    "profession": profession,
                    "position": position,
                    "tags": tags,
                    "url": f"{BASE_URL}/w/{quote(name)}",
                })
                g["stars"].append(star)

        if not groups:
            return (
                "# ğŸ” å…¬æ‹›è®¡ç®—ï¼ˆåˆ†ç»„ï¼‰\n\n"
                f"- **è¯æ¡**: {', '.join(selected) if selected else 'æ— '}\n"
                "- æš‚æ— å¯åŒ¹é…åˆ†ç»„ï¼ˆå¯èƒ½æ˜¯è¯ç¼€è¿‡äºè‹›åˆ»æˆ–æœªé€‰æ‹©ä»»ä½•å¯åˆ†ç»„çš„è¯æ¡ï¼‰ã€‚\n"
            )

        # æ’åºåˆ†ç»„
        def group_score(item):
            subset, g = item
            avg = sum(g["stars"]) / max(1, len(g["stars"]))
            return (avg, len(g["ops"]))

        ordered = sorted(groups.items(), key=group_score, reverse=True)

        # æ¸²æŸ“
        def subset_to_names(subset: int) -> List[str]:
            names = []
            i = 0
            while (1 << i) <= subset:
                if subset & (1 << i):
                    names.append(R[i])
                i += 1
            return names

        lines = ["# ğŸ¯ å…¬æ‹›è®¡ç®—ç»“æœï¼ˆåˆ†ç»„ï¼‰", "", f"- **è¯æ¡**: {', '.join(selected)}", ""]
        for subset, g in ordered:
            title = "+".join(subset_to_names(subset)) or "(æœªå‘½åå­é›†)"
            lines.append(f"## {title}")
            # åˆ—è¡¨ï¼šæŒ‰æ˜Ÿçº§é™åº
            ops_sorted = sorted(g["ops"], key=lambda x: (-x["star"], x["profession"], x["name"]))
            for op in ops_sorted:
                meta = [f"{op['star']}â˜…", op["profession"], op["position"]]
                lines.append(f"- **{op['name']}**ï¼ˆ{' / '.join([m for m in meta if m])}ï¼‰")
                lines.append(f"  é“¾æ¥: {op['url']}")
            lines.append("")

        lines.append("---")
        lines.append("æ•°æ®æ¥æº: å…¬æ‹›è®¡ç®— / cargoqueryï¼ˆåˆ†ç»„é€»è¾‘ä¸å‰ç«¯ä¸€è‡´ï¼‰")
        return "\n".join(lines)

    except Exception as e:
        return f"# âŒ å…¬æ‹›è®¡ç®—ï¼ˆåˆ†ç»„ï¼‰é”™è¯¯\n\n- **é”™è¯¯ä¿¡æ¯**: {e}"
    finally:
        if not client_provided:
            await wiki_client.close() 